{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 1X9LyYMW3fNLTb7iQSsYYvguttE_NAhptrGLdA1Hdon1cZPa"
      ],
      "metadata": {
        "id": "h1nqCZJd1h-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn nest-asyncio pyngrok"
      ],
      "metadata": {
        "id": "UzqzdhCk19lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 1X9LyYMW3fNLTb7iQSsYYvguttE_NAhptrGLdA1Hdon1cZPa"
      ],
      "metadata": {
        "id": "6CFo7zeo19iG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "\n",
        "# #Mount Google Drive if not already mounted\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # Define the directory where the files are saved\n",
        "# save_dir = \"/content/drive/MyDrive/models/\"\n",
        "\n",
        "# #Load the LSTM model (you can use the legacy h5 file format or convert to the new keras format)\n",
        "# lstm_model = tf.keras.models.load_model(save_dir + \"lstm_yield_model.h5\")\n",
        "# print(\"LSTM model loaded successfully.\")\n",
        "\n",
        "# # Load the feature scaler (scaler_X)\n",
        "# with open(save_dir + \"scaler_X.pkl\", \"rb\") as f:\n",
        "#     scaler_X = pickle.load(f)\n",
        "# print(\"Feature scaler loaded successfully.\")\n",
        "\n",
        "# # Load the target scaler (scaler_y)\n",
        "# with open(save_dir + \"scaler_y.pkl\", \"rb\") as f:\n",
        "#     scaler_y = pickle.load(f)\n",
        "# print(\"Target scaler loaded successfully.\")\n"
      ],
      "metadata": {
        "id": "C90d3T1S1fiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1# Import necessary libraries\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nest_asyncio\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "import tensorflow as tf\n",
        "\n",
        "# Apply nest_asyncio to allow using asyncio in notebook environments\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# -----------------------------\n",
        "# FastAPI App Setup\n",
        "# -----------------------------\n",
        "app = FastAPI(\n",
        "    title=\"Agriculture AI Models API\",\n",
        "    description=\"API for agricultural prediction models\"\n",
        ")\n",
        "\n",
        "# Enable CORS middleware (allowing requests from any origin)\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Define Input Models\n",
        "# -----------------------------\n",
        "# class YieldPredictionInput(BaseModel):\n",
        "#     latitude: float\n",
        "#     longitude: float\n",
        "#     crop_type: str\n",
        "#     target_year: int  # The year for which yield is to be predicted\n",
        "\n",
        "# # -----------------------------\n",
        "# Load Pre-Trained LSTM Model and Scalers from Google Drive\n",
        "# -----------------------------\n",
        "# from google.colab import drive\n",
        "\n",
        "# # Mount Google Drive if not already mounted\n",
        "# drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# # Define the correct path for model and scaler files in Google Drive\n",
        "# save_dir = \"/content/drive/MyDrive/models/\"  # Update if your files are in another folder\n",
        "\n",
        "# # try:\n",
        "#    lstm_model = tf.keras.models.load_model(save_dir + \"lstm_yield_model.h5\")\n",
        "#    with open(save_dir + \"scaler_X.pkl\", \"rb\") as f:\n",
        "#        scaler_X = pickle.load(f)\n",
        "#    with open(save_dir + \"scaler_y.pkl\", \"rb\") as f:\n",
        "#        scaler_y = pickle.load(f)\n",
        "\n",
        "#     print(\"✅ LSTM model and scalers loaded successfully from Google Drive.\")\n",
        "# #except Exception as e:\n",
        "#     print(\"❌ Error loading LSTM model or scalers:\", str())\n",
        "#     lstm_model = None\n",
        "\n",
        "# -----------------------------\n",
        "# Dummy Function to Retrieve Past Features\n",
        "# -----------------------------\n",
        "def get_past_year_features(latitude, longitude, crop_type, target_year, window_size=3):\n",
        "    \"\"\"\n",
        "    Dummy function to simulate retrieval of the past 'window_size' years\n",
        "    of remote sensing features (e.g., NDVI, EVI, LSWI) for the given location and crop.\n",
        "\n",
        "    In a real application, this function would query a database or use an API\n",
        "    to fetch historical data based on the provided coordinates and crop type.\n",
        "\n",
        "    For demonstration, we return a constant array.\n",
        "    \"\"\"\n",
        "    # Assume we have 3 features: NDVI, EVI, LSWI\n",
        "    n_features = 3\n",
        "    # Create a dummy sequence (for example, use constant values)\n",
        "    # Here, we assume that for the previous window_size years, the features are as follows:\n",
        "    dummy_sequence = np.array([[0.65, 0.45, 0.20]] * window_size)  # shape: (window_size, n_features)\n",
        "    return dummy_sequence\n",
        "\n",
        "# -----------------------------\n",
        "# Define API Endpoints\n",
        "# -----------------------------\n",
        "@app.get(\"/\")\n",
        "def read_root():\n",
        "    return {\n",
        "        \"message\": \"Welcome to Agriculture AI Models API\",\n",
        "        \"available_endpoints\": [\"/predict/crop-yield\", \"/predict/water-requirement\", \"/predict/soil-analysis\", \"/predict/disease-detection\"]\n",
        "    }\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health_check():\n",
        "    \"\"\"Health check endpoint to verify the API is running.\"\"\"\n",
        "    return {\"status\": \"healthy\", \"timestamp\": time.time()}\n",
        "\n",
        "# @app.post(\"/predict/crop-yield\")\n",
        "# def predict_crop_yield(input_data: YieldPredictionInput):\n",
        "#     \"\"\"\n",
        "#     Predicts crop yield using the pre-trained LSTM time-series model.\n",
        "\n",
        "#     Input:\n",
        "#       - latitude, longitude: Coordinates of the field.\n",
        "#       - crop_type: The crop for which yield is to be predicted.\n",
        "#       - target_year: The year for which prediction is required.\n",
        "\n",
        "#     Process:\n",
        "#       1. Retrieve the past years’ features using a dummy function (or database/API in production).\n",
        "#       2. Normalize the feature sequence using scaler_X.\n",
        "#       3. Reshape the sequence to match the LSTM input shape.\n",
        "#       4. Use the LSTM model to predict the scaled yield.\n",
        "#       5. Inverse-transform the prediction to get the yield in original units.\n",
        "\n",
        "#     Returns:\n",
        "#       - The predicted yield in tons per hectare.\n",
        "#     \"\"\"\n",
        "#     if lstm_model is None:\n",
        "#         raise HTTPException(status_code=500, detail=\"Yield prediction model is not available.\")\n",
        "\n",
        "#     # Step 1: Retrieve past sequence of features for the given coordinates and crop.\n",
        "#     window_size = 3  # As used in model training\n",
        "#     sequence = get_past_year_features(\n",
        "#         latitude=input_data.latitude,\n",
        "#         longitude=input_data.longitude,\n",
        "#         crop_type=input_data.crop_type,\n",
        "#         target_year=input_data.target_year,\n",
        "#         window_size=window_size\n",
        "#     )\n",
        "#     # Debug print: shape of retrieved sequence\n",
        "#     print(\"Retrieved sequence shape:\", sequence.shape)\n",
        "\n",
        "#     # Step 2: Normalize the sequence using the loaded scaler_X.\n",
        "#     # Reshape sequence to 2D array for scaling then back to 3D.\n",
        "#     n_features = sequence.shape[1]\n",
        "#     sequence_flat = sequence.reshape(-1, n_features)\n",
        "#     sequence_scaled_flat = scaler_X.transform(sequence_flat)\n",
        "#     sequence_scaled = sequence_scaled_flat.reshape(1, window_size, n_features)  # LSTM expects shape (1, window_size, n_features)\n",
        "\n",
        "#     # Step 3: Predict yield using the LSTM model.\n",
        "#     y_pred_scaled = lstm_model.predict(sequence_scaled)\n",
        "#     y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
        "\n",
        "#     # Return prediction as response.\n",
        "#     return {\n",
        "#         \"crop_type\": input_data.crop_type,\n",
        "#         \"target_year\": input_data.target_year,\n",
        "#         \"latitude\": input_data.latitude,\n",
        "#         \"longitude\": input_data.longitude,\n",
        "#         \"predicted_yield\": float(y_pred[0][0]),\n",
        "#         \"unit\": \"tons per hectare\"\n",
        "#     }\n",
        "\n",
        "# -----------------------------\n",
        "# Function to Start the Server (for Notebook/Local Use)\n",
        "# -----------------------------\n",
        "def start_server():\n",
        "    # Start ngrok tunnel for external access (useful for Colab or local testing)\n",
        "    ngrok_tunnel = ngrok.connect(8000)\n",
        "    print(f\"Public URL: {ngrok_tunnel.public_url}\")\n",
        "    print(f\"API docs available at: {ngrok_tunnel.public_url}/docs\")\n",
        "\n",
        "    # Run the FastAPI server\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "# Uncomment the line below to start the server if running as main module\n",
        "# if __name__ == \"__main__\":\n",
        "#  start_server()\n"
      ],
      "metadata": {
        "id": "uX2C51ZhgCyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi.responses import JSONResponse"
      ],
      "metadata": {
        "id": "L4v5IWnXxliP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***FastAPI Integration***"
      ],
      "metadata": {
        "id": "tDoTlsUJdLck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CHSPredictionInput(BaseModel):\n",
        "    end_year: int\n",
        "    start_year: int\n",
        "    x_min: float\n",
        "    y_min: float\n",
        "    x_max: float\n",
        "    y_max: float"
      ],
      "metadata": {
        "id": "6Kss6SshdJwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@app.post(\"/predict/ndvi\")\n",
        "def predict_ndvi(input_data: CHSPredictionInput):\n",
        "    try:\n",
        "        df = create_ndvi_dataset(input_data.start_year, input_data.end_year, input_data.x_min, input_data.y_min, input_data.x_max, input_data.y_max)\n",
        "        df = interpolate_ndvi(df)\n",
        "        df = df.dropna()\n",
        "        df = df.reset_index(drop=True)\n",
        "        data = train_and_evaluate(df)\n",
        "        return JSONResponse(content={\"ndvi_values\": data})\n",
        "    except Exception as e:\n",
        "        print(f\"Error in predict_ndvi: {e}\")  # Print the error to the console\n",
        "        raise HTTPException(status_code=500, detail=str(e))  # Return a detailed error response"
      ],
      "metadata": {
        "id": "PR2LIKS-kdCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@app.post(\"/predict/ndwi\")\n",
        "def predict_ndwi(input_data: CHSPredictionInput):\n",
        "  df = create_ndwi_dataset(input_data.start_year, input_data.end_year, input_data.x_min , input_data.y_min , input_data.x_max , input_data.y_max)\n",
        "  df = interpolate_ndwi(df)\n",
        "  df = df.dropna()\n",
        "  df = df.reset_index(drop=True)\n",
        "\n",
        "  bilstm_train_metrics, bilstm_test_metrics, bilstm_model, bilstm_history = train_and_evaluate(df, model_type='bilstm')\n",
        "\n",
        "  data = predict_future_ndwi(df, bilstm_model)\n",
        "  return JSONResponse(content={\"ndwi_values\": data})"
      ],
      "metadata": {
        "id": "GkT9LD_5bZS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NDWI Model**"
      ],
      "metadata": {
        "id": "dm3PcmV9f9cy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ee\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project = 'agrisage-454715')"
      ],
      "metadata": {
        "id": "cq4c6Apmc-eS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_full_timestamp_range(start_year, end_year):\n",
        "    \"\"\"\n",
        "    Generate complete timestamp range for wheat growing season\n",
        "    \"\"\"\n",
        "    timestamps = []\n",
        "    for year in range(start_year, end_year + 1):\n",
        "        # Wheat growing season from Nov to May\n",
        "        current_date = datetime(year, 11, 1)\n",
        "        season_end = datetime(year + 1, 5, 31)\n",
        "\n",
        "        while current_date <= season_end:\n",
        "            timestamps.append(current_date)\n",
        "            current_date += timedelta(days=10)\n",
        "\n",
        "    return timestamps\n",
        "\n",
        "def load_ndwi_for_period(start_date, end_date, region):\n",
        "    \"\"\"\n",
        "    Load Sentinel images and calculate NDWI for specific period\n",
        "    \"\"\"\n",
        "    # Convert datetime to ee.Date\n",
        "    start = ee.Date(start_date.strftime('%Y-%m-%d'))\n",
        "    end = ee.Date(end_date.strftime('%Y-%m-%d'))\n",
        "\n",
        "    # Load Sentinel-2 images\n",
        "    collection = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
        "        .filterDate(start, end) \\\n",
        "        .filterBounds(region) \\\n",
        "        .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30))\n",
        "\n",
        "    # If no images, return None\n",
        "    if collection.size().getInfo() == 0:\n",
        "        return None\n",
        "\n",
        "    # Calculate NDWI composite\n",
        "    ndwi_composite = collection \\\n",
        "        .map(lambda image: image.normalizedDifference(['B3', 'B8']).rename('NDWI')) \\\n",
        "        .mean()\n",
        "\n",
        "    # Extract mean NDWI value\n",
        "    try:\n",
        "        mean_ndwi = ndwi_composite.reduceRegion(\n",
        "            reducer=ee.Reducer.mean(),\n",
        "            geometry=region,\n",
        "            scale=10,\n",
        "            maxPixels=1e9\n",
        "        ).get('NDWI').getInfo()\n",
        "        return mean_ndwi\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def create_ndwi_dataset(start_year, end_year, x_min , y_min, x_max , y_max ):\n",
        "    \"\"\"\n",
        "    Create comprehensive NDWI dataset\n",
        "    \"\"\"\n",
        "  # Define region of interest (example: Punjab, India)\n",
        "    xmin, ymin = x_min, y_min\n",
        "    xmax, ymax = x_max, y_max\n",
        "    punjab = ee.Geometry.Rectangle([x_min, y_min,x_max, y_max])\n",
        "\n",
        "    # Generate full timestamp range\n",
        "    timestamps = generate_full_timestamp_range(start_year, end_year)\n",
        "\n",
        "    # Prepare data collection\n",
        "    data = []\n",
        "    for timestamp in timestamps:\n",
        "        # 10-day period\n",
        "        period_end = timestamp + timedelta(days=10)\n",
        "\n",
        "        # Calculate NDWI\n",
        "        ndwi = load_ndwi_for_period(timestamp, period_end, punjab)\n",
        "\n",
        "        data.append({\n",
        "            'timestamp': timestamp,\n",
        "            'NDWIi': ndwi\n",
        "        })\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "    df['year'] = df['timestamp'].dt.year\n",
        "    df['month'] = df['timestamp'].dt.month\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "RZDEs5_xckV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def interpolate_ndwi(df):\n",
        "    \"\"\"\n",
        "    Interpolate NDWI values with improved seasonal handling and edge cases\n",
        "    \"\"\"\n",
        "    # Create a copy to avoid modifying original data\n",
        "    df_filtered = df.copy()\n",
        "\n",
        "    # Convert timestamp to datetime if it isn't already\n",
        "    df_filtered['timestamp'] = pd.to_datetime(df_filtered['timestamp'])\n",
        "\n",
        "    # Create seasonal features\n",
        "    df_filtered['month'] = df_filtered['timestamp'].dt.month\n",
        "    df_filtered['day'] = df_filtered['timestamp'].dt.day\n",
        "    df_filtered['day_of_year'] = df_filtered['timestamp'].dt.dayofyear\n",
        "\n",
        "    # Calculate seasonal medians for each day-month combination\n",
        "    seasonal_medians = df_filtered.groupby(['month', 'day'])['NDWIi'].transform('median')\n",
        "\n",
        "    # First pass: Fill missing values with seasonal medians\n",
        "    df_filtered['NDWIi_interpolated'] = df_filtered['NDWIi'].fillna(seasonal_medians)\n",
        "\n",
        "    # Second pass: Apply linear interpolation within each year\n",
        "    for year in df_filtered['year'].unique():\n",
        "        year_mask = df_filtered['year'] == year\n",
        "        df_filtered.loc[year_mask, 'NDWIi_interpolated'] = (\n",
        "            df_filtered.loc[year_mask, 'NDWIi_interpolated']\n",
        "            .interpolate(method='linear', limit_direction='forward', limit=30)  # Limit to 30 days forward\n",
        "        )\n",
        "\n",
        "    # Third pass: Fill any remaining NaN with seasonal patterns\n",
        "    remaining_nans = df_filtered['NDWIi_interpolated'].isna()\n",
        "    if remaining_nans.any():\n",
        "        # Calculate the average NDWI pattern across all years\n",
        "        seasonal_pattern = (\n",
        "            df_filtered.groupby('day_of_year')['NDWIi']\n",
        "            .mean()\n",
        "            .interpolate(method='cubic')  # Smooth the seasonal pattern\n",
        "        )\n",
        "\n",
        "        # Fill remaining NaNs with the seasonal pattern\n",
        "        for idx in df_filtered[remaining_nans].index:\n",
        "            day_of_year = df_filtered.loc[idx, 'day_of_year']\n",
        "            df_filtered.loc[idx, 'NDWIi_interpolated'] = seasonal_pattern[day_of_year]\n",
        "\n",
        "        return df_filtered"
      ],
      "metadata": {
        "id": "GqelkMrXc5A4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, GRU\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:(i + seq_length)])\n",
        "        y.append(data[i + seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def prepare_data(df, seq_length=6):\n",
        "    train_data = df[df['year'] < 2024]['NDWIi_interpolated'].values\n",
        "    test_data = df[df['year'] >= 2024]['NDWIi_interpolated'].values\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    train_scaled = scaler.fit_transform(train_data.reshape(-1, 1))\n",
        "    test_scaled = scaler.transform(test_data.reshape(-1, 1))\n",
        "\n",
        "    X_train, y_train = create_sequences(train_scaled, seq_length)\n",
        "    X_test, y_test = create_sequences(test_scaled, seq_length)\n",
        "\n",
        "    return X_train, y_train, X_test, y_test, scaler\n",
        "\n",
        "def build_bilstm_model(seq_length):\n",
        "    model = Sequential([\n",
        "        Bidirectional(LSTM(64, activation='relu', return_sequences=True),\n",
        "                     input_shape=(seq_length, 1)),\n",
        "        Dropout(0.2),\n",
        "        Bidirectional(LSTM(32, activation='relu')),\n",
        "        Dropout(0.2),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "def evaluate_predictions(y_true, y_pred):\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    accuracy = 1 - (rmse / (np.max(y_true) - np.min(y_true)))\n",
        "\n",
        "    return {\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'R2': r2,\n",
        "        'Accuracy': accuracy\n",
        "    }\n",
        "\n",
        "def train_and_evaluate(df, model_type='bilstm'):\n",
        "    # Prepare data\n",
        "    X_train, y_train, X_test, y_test, scaler = prepare_data(df)\n",
        "\n",
        "    # Select model type\n",
        "    if model_type.lower() == 'bilstm':\n",
        "        model = build_bilstm_model(X_train.shape[1])\n",
        "        model_name = 'Bidirectional LSTM'\n",
        "    else:\n",
        "        raise ValueError(\"Model type must be 'bilstm'\")\n",
        "\n",
        "    # Train model\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=100,\n",
        "        batch_size=32,\n",
        "        validation_split=0.2,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Make predictions\n",
        "    train_pred = model.predict(X_train)\n",
        "    test_pred = model.predict(X_test)\n",
        "\n",
        "    # Inverse transform predictions\n",
        "    train_pred = scaler.inverse_transform(train_pred)\n",
        "    test_pred = scaler.inverse_transform(test_pred)\n",
        "    y_train_orig = scaler.inverse_transform(y_train)\n",
        "    y_test_orig = scaler.inverse_transform(y_test)\n",
        "\n",
        "    # Evaluate model\n",
        "    train_metrics = evaluate_predictions(y_train_orig, train_pred)\n",
        "    test_metrics = evaluate_predictions(y_test_orig, test_pred)\n",
        "\n",
        "    return train_metrics, test_metrics, model, history"
      ],
      "metadata": {
        "id": "ibtpm5t5dRm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forecast_future(model, last_sequence, scaler, n_days=10):\n",
        "    \"\"\"\n",
        "    Forecast future NDWI values using the trained Bi-LSTM model\n",
        "\n",
        "    Parameters:\n",
        "    model: trained Bi-LSTM model\n",
        "    last_sequence: last known sequence of NDWI values\n",
        "    scaler: fitted MinMaxScaler\n",
        "    n_days: number of days to forecast\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    current_sequence = last_sequence.copy()\n",
        "\n",
        "    # Make predictions for n_days\n",
        "    for _ in range(n_days):\n",
        "        # Reshape sequence for model input\n",
        "        current_sequence_scaled = scaler.transform(current_sequence.reshape(-1, 1))\n",
        "        X = current_sequence_scaled.reshape(1, len(current_sequence), 1)\n",
        "\n",
        "        # Predict next value\n",
        "        next_pred_scaled = model.predict(X, verbose=0)\n",
        "        next_pred = scaler.inverse_transform(next_pred_scaled)[0][0]\n",
        "\n",
        "        # Append prediction\n",
        "        predictions.append(next_pred)\n",
        "\n",
        "        # Update sequence for next prediction\n",
        "        current_sequence = np.roll(current_sequence, -1)\n",
        "        current_sequence[-1] = next_pred\n",
        "\n",
        "    return np.array(predictions)"
      ],
      "metadata": {
        "id": "A5svc1fId69e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_future_ndwi(df, trained_model, seq_length=6, forecast_days=1):\n",
        "    \"\"\"\n",
        "    Main function to prepare data and make future predictions\n",
        "    \"\"\"\n",
        "    # Prepare the last known sequence\n",
        "    last_known_values = df['NDWIi_interpolated'].values[-seq_length:]\n",
        "\n",
        "    # Get the scaler\n",
        "    all_values = df['NDWIi_interpolated'].values\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(all_values.reshape(-1, 1))\n",
        "\n",
        "    # Make predictions\n",
        "    future_predictions = forecast_future(trained_model, last_known_values,\n",
        "                                       scaler, forecast_days)\n",
        "\n",
        "    # Create dates for predictions\n",
        "    last_date = pd.to_datetime(df['timestamp'].iloc[-1])\n",
        "    future_dates = [last_date + pd.Timedelta(days=i+1) for i in range(forecast_days)]\n",
        "\n",
        "    # Create forecast DataFrame\n",
        "    forecast_df = pd.DataFrame({\n",
        "        'Date': future_dates,\n",
        "        'Predicted_NDWI': future_predictions\n",
        "    })\n",
        "\n",
        "    return forecast_df"
      ],
      "metadata": {
        "id": "8Wpj1JdiebBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NDVI MODEL**"
      ],
      "metadata": {
        "id": "og6-MZIdfxtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import ee\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project = 'agrisage-454715')\n",
        "\n",
        "def generate_full_timestamp_range(start_year, end_year):\n",
        "    \"\"\"\n",
        "    Generate complete timestamp range for wheat growing season\n",
        "    \"\"\"\n",
        "    timestamps = []\n",
        "    for year in range(start_year, end_year + 1):\n",
        "        # Wheat growing season from Nov to May\n",
        "        current_date = datetime(year, 11, 1)\n",
        "        season_end = datetime(year + 1, 5, 31)\n",
        "\n",
        "        while current_date <= season_end:\n",
        "            timestamps.append(current_date)\n",
        "            current_date += timedelta(days=10)\n",
        "\n",
        "    return timestamps\n",
        "\n",
        "def load_ndvi_for_period(start_date, end_date, region):\n",
        "\n",
        "    # Convert datetime to ee.Date\n",
        "    start = ee.Date(start_date.strftime('%Y-%m-%d'))\n",
        "    end = ee.Date(end_date.strftime('%Y-%m-%d'))\n",
        "\n",
        "    # Load Sentinel-2 images\n",
        "    collection = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
        "        .filterDate(start, end) \\\n",
        "        .filterBounds(region) \\\n",
        "        .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30))\n",
        "\n",
        "    # If no images, return None\n",
        "    if collection.size().getInfo() == 0:\n",
        "        return None\n",
        "\n",
        "    # Calculate NDVI composite\n",
        "    ndvi_composite = collection \\\n",
        "        .map(lambda image: image.normalizedDifference(['B8', 'B4']).rename('NDVI')) \\\n",
        "        .mean()\n",
        "\n",
        "    # Extract mean NDVI value\n",
        "    try:\n",
        "        mean_ndvi = ndvi_composite.reduceRegion(\n",
        "            reducer=ee.Reducer.mean(),\n",
        "            geometry=region,\n",
        "            scale=10,\n",
        "            maxPixels=1e9\n",
        "        ).get('NDVI').getInfo()\n",
        "        return mean_ndvi\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def create_ndvi_dataset(start_year, end_year, x_min , y_min, x_max , y_max ):\n",
        "    \"\"\"\n",
        "    Create comprehensive NDVI dataset\n",
        "    \"\"\"\n",
        "  # Define region of interest (example: Punjab, India)\n",
        "    xmin, ymin = x_min, y_min\n",
        "    xmax, ymax = x_max, y_max\n",
        "    punjab = ee.Geometry.Rectangle([x_min, y_min,x_max, y_max])\n",
        "\n",
        "\n",
        "    # Generate full timestamp range\n",
        "    timestamps = generate_full_timestamp_range(start_year, end_year)\n",
        "\n",
        "    # Prepare data collection\n",
        "    data = []\n",
        "    for timestamp in timestamps:\n",
        "        # 10-day period\n",
        "        period_end = timestamp + timedelta(days=10)\n",
        "\n",
        "        # Calculate NDVI\n",
        "        ndvi = load_ndvi_for_period(timestamp, period_end, punjab)\n",
        "\n",
        "        data.append({\n",
        "            'timestamp': timestamp,\n",
        "            'NDVIi': ndvi\n",
        "        })\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "    df['year'] = df['timestamp'].dt.year\n",
        "    df['month'] = df['timestamp'].dt.month\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "qFj_qrb-efvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def interpolate_ndvi(df):\n",
        "\n",
        "    # Create a copy to avoid modifying original data\n",
        "    df_filtered = df.copy()\n",
        "\n",
        "    # Convert timestamp to datetime if it isn't already\n",
        "    df_filtered['timestamp'] = pd.to_datetime(df_filtered['timestamp'])\n",
        "\n",
        "    # Create seasonal features\n",
        "    df_filtered['month'] = df_filtered['timestamp'].dt.month\n",
        "    df_filtered['day'] = df_filtered['timestamp'].dt.day\n",
        "    df_filtered['day_of_year'] = df_filtered['timestamp'].dt.dayofyear\n",
        "\n",
        "    # Calculate seasonal medians for each day-month combination\n",
        "    seasonal_medians = df_filtered.groupby(['month', 'day'])['NDVIi'].transform('median')\n",
        "\n",
        "    # First pass: Fill missing values with seasonal medians\n",
        "    df_filtered['NDVIi_interpolated'] = df_filtered['NDVIi'].fillna(seasonal_medians)\n",
        "\n",
        "    # Second pass: Apply linear interpolation within each year\n",
        "    for year in df_filtered['year'].unique():\n",
        "        year_mask = df_filtered['year'] == year\n",
        "        df_filtered.loc[year_mask, 'NDVIi_interpolated'] = (\n",
        "            df_filtered.loc[year_mask, 'NDVIi_interpolated']\n",
        "            .interpolate(method='linear', limit_direction='forward', limit=30)  # Limit to 30 days forward\n",
        "        )\n",
        "\n",
        "    # Third pass: Fill any remaining NaN with seasonal patterns\n",
        "    remaining_nans = df_filtered['NDVIi_interpolated'].isna()\n",
        "    if remaining_nans.any():\n",
        "        # Calculate the average NDVI pattern across all years\n",
        "        seasonal_pattern = (\n",
        "            df_filtered.groupby('day_of_year')['NDVIi']\n",
        "            .mean()\n",
        "            .interpolate(method='cubic')  # Smooth the seasonal pattern\n",
        "        )\n",
        "\n",
        "        # Fill remaining NaNs with the seasonal pattern\n",
        "        for idx in df_filtered[remaining_nans].index:\n",
        "            day_of_year = df_filtered.loc[idx, 'day_of_year']\n",
        "            df_filtered.loc[idx, 'NDVIi_interpolated'] = seasonal_pattern[day_of_year]\n",
        "\n",
        "    return df_filtered"
      ],
      "metadata": {
        "id": "tI77XMohe8A1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import timedelta\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:(i + seq_length)])\n",
        "        y.append(data[i + seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def prepare_data(df, seq_length=6):\n",
        "    train_data = df[df['year'] < 2024]['NDVIi_interpolated'].values\n",
        "    test_data = df[df['year'] >= 2024]['NDVIi_interpolated'].values\n",
        "    test_dates = df[df['year'] >= 2024]['timestamp'].values  # Get test set dates\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    train_scaled = scaler.fit_transform(train_data.reshape(-1, 1))\n",
        "    test_scaled = scaler.transform(test_data.reshape(-1, 1))\n",
        "\n",
        "    X_train, y_train = create_sequences(train_scaled, seq_length)\n",
        "    X_test, y_test = create_sequences(test_scaled, seq_length)\n",
        "\n",
        "    return X_train, y_train, X_test, y_test, scaler, test_dates[-1]\n",
        "\n",
        "def build_lstm_model(seq_length):\n",
        "    model = Sequential([\n",
        "        LSTM(64, activation='relu', input_shape=(seq_length, 1), return_sequences=True),\n",
        "        Dropout(0.2),\n",
        "        LSTM(32, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "def evaluate_predictions(y_true, y_pred):\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    # Calculate accuracy as 1 - normalized RMSE\n",
        "    accuracy = 1 - (rmse / (np.max(y_true) - np.min(y_true)))\n",
        "\n",
        "    return {\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'R2': r2,\n",
        "        'Accuracy': accuracy\n",
        "    }\n",
        "\n",
        "# Predict next 10 days NDVI\n",
        "\n",
        "def predict_next_10_days(model, last_sequence, scaler, last_date):\n",
        "    predictions = []\n",
        "    dates = []\n",
        "    input_seq = last_sequence.copy()\n",
        "\n",
        "    for i in range(1):  # Predict for next 10 days which will be calculted to give as a new predicted ndvi composite\n",
        "        pred = model.predict(input_seq.reshape(1, -1, 1))\n",
        "        pred_inv = scaler.inverse_transform(pred)\n",
        "        predictions.append(pred_inv[0][0])\n",
        "\n",
        "        # Convert last_date to datetime if needed\n",
        "        last_date = pd.to_datetime(last_date)\n",
        "        dates.append(last_date + timedelta(days=i + 1))  # Generate future timestamps\n",
        "\n",
        "        input_seq = np.append(input_seq[1:], pred)\n",
        "\n",
        "    return predictions, dates\n",
        "\n",
        "\n",
        "# Main execution\n",
        "def train_and_evaluate(df):\n",
        "    # Prepare data\n",
        "    X_train, y_train, X_test, y_test, scaler,last_test_date = prepare_data(df)\n",
        "\n",
        "    # Build and train model\n",
        "    model = build_lstm_model(X_train.shape[1])\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=100,\n",
        "        batch_size=32,\n",
        "        validation_split=0.2,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Make predictions\n",
        "    train_pred = model.predict(X_train)\n",
        "    test_pred = model.predict(X_test)\n",
        "\n",
        "    # Inverse transform predictions\n",
        "    train_pred = scaler.inverse_transform(train_pred)\n",
        "    test_pred = scaler.inverse_transform(test_pred)\n",
        "    y_train_orig = scaler.inverse_transform(y_train)\n",
        "    y_test_orig = scaler.inverse_transform(y_test)\n",
        "\n",
        "    # Evaluate model\n",
        "    train_metrics = evaluate_predictions(y_train_orig, train_pred)\n",
        "    test_metrics = evaluate_predictions(y_test_orig, test_pred)\n",
        "\n",
        "    # Predict next 10 days NDVI\n",
        "    last_sequence = X_test[-1]  # Get last sequence from test data\n",
        "    next_10_days_predictions, next_10_days_dates = predict_next_10_days(model, last_sequence, scaler, last_test_date)\n",
        "\n",
        "    global global_next_10_days_prediction\n",
        "    global_next_10_days_prediction = list(next_10_days_predictions)\n",
        "\n",
        "    return global_next_10_days_prediction"
      ],
      "metadata": {
        "id": "6PSMcbQdfMdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert(df):\n",
        "  new_df = df.copy()\n",
        "\n"
      ],
      "metadata": {
        "id": "nSlSGvDbo2el"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_server()"
      ],
      "metadata": {
        "id": "UlbWZAR83cRo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88abc582-b388-4da3-9ac3-159e51b2dfc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: https://615d-34-125-101-193.ngrok-free.app\n",
            "API docs available at: https://615d-34-125-101-193.ngrok-free.app/docs\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [465]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     2409:40e3:2093:753a:a9c5:d501:107:3738:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40e3:2093:753a:a9c5:d501:107:3738:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step - loss: 0.4465 - val_loss: 0.1102\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - loss: 0.4343 - val_loss: 0.1069\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 0.4246 - val_loss: 0.1040\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.4180 - val_loss: 0.1010\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.4087 - val_loss: 0.0981\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - loss: 0.4006 - val_loss: 0.0952\n",
            "Epoch 7/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.3900 - val_loss: 0.0923\n",
            "Epoch 8/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.3798 - val_loss: 0.0894\n",
            "Epoch 9/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 0.3751 - val_loss: 0.0864\n",
            "Epoch 10/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.3630 - val_loss: 0.0834\n",
            "Epoch 11/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 0.3538 - val_loss: 0.0803\n",
            "Epoch 12/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.3453 - val_loss: 0.0773\n",
            "Epoch 13/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.3280 - val_loss: 0.0742\n",
            "Epoch 14/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.3200 - val_loss: 0.0712\n",
            "Epoch 15/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.3083 - val_loss: 0.0683\n",
            "Epoch 16/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.3023 - val_loss: 0.0654\n",
            "Epoch 17/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.2950 - val_loss: 0.0627\n",
            "Epoch 18/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.2767 - val_loss: 0.0601\n",
            "Epoch 19/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.2672 - val_loss: 0.0579\n",
            "Epoch 20/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.2542 - val_loss: 0.0559\n",
            "Epoch 21/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.2464 - val_loss: 0.0543\n",
            "Epoch 22/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.2236 - val_loss: 0.0533\n",
            "Epoch 23/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.2124 - val_loss: 0.0530\n",
            "Epoch 24/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.1884 - val_loss: 0.0536\n",
            "Epoch 25/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.1891 - val_loss: 0.0555\n",
            "Epoch 26/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 0.1854 - val_loss: 0.0588\n",
            "Epoch 27/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.1637 - val_loss: 0.0639\n",
            "Epoch 28/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.1621 - val_loss: 0.0709\n",
            "Epoch 29/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 0.1642 - val_loss: 0.0795\n",
            "Epoch 30/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.1604 - val_loss: 0.0890\n",
            "Epoch 31/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.1473 - val_loss: 0.0986\n",
            "Epoch 32/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1405 - val_loss: 0.1063\n",
            "Epoch 33/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 0.1677 - val_loss: 0.1107\n",
            "Epoch 34/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.1608 - val_loss: 0.1120\n",
            "Epoch 35/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.1594 - val_loss: 0.1110\n",
            "Epoch 36/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.1539 - val_loss: 0.1080\n",
            "Epoch 37/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1698 - val_loss: 0.1037\n",
            "Epoch 38/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.1666 - val_loss: 0.0985\n",
            "Epoch 39/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1482 - val_loss: 0.0928\n",
            "Epoch 40/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.1485 - val_loss: 0.0879\n",
            "Epoch 41/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.1291 - val_loss: 0.0835\n",
            "Epoch 42/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.1399 - val_loss: 0.0797\n",
            "Epoch 43/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1444 - val_loss: 0.0769\n",
            "Epoch 44/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.1506 - val_loss: 0.0747\n",
            "Epoch 45/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.1237 - val_loss: 0.0732\n",
            "Epoch 46/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 0.1449 - val_loss: 0.0721\n",
            "Epoch 47/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - loss: 0.1218 - val_loss: 0.0717\n",
            "Epoch 48/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1364 - val_loss: 0.0717\n",
            "Epoch 49/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.1320 - val_loss: 0.0721\n",
            "Epoch 50/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1341 - val_loss: 0.0728\n",
            "Epoch 51/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1335 - val_loss: 0.0738\n",
            "Epoch 52/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.1478 - val_loss: 0.0753\n",
            "Epoch 53/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - loss: 0.1292 - val_loss: 0.0767\n",
            "Epoch 54/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.1298 - val_loss: 0.0785\n",
            "Epoch 55/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1502 - val_loss: 0.0802\n",
            "Epoch 56/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 0.1332 - val_loss: 0.0823\n",
            "Epoch 57/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.1372 - val_loss: 0.0845\n",
            "Epoch 58/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 0.1162 - val_loss: 0.0869\n",
            "Epoch 59/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.1306 - val_loss: 0.0892\n",
            "Epoch 60/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 0.1323 - val_loss: 0.0911\n",
            "Epoch 61/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.1040 - val_loss: 0.0931\n",
            "Epoch 62/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.1224 - val_loss: 0.0945\n",
            "Epoch 63/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 0.1269 - val_loss: 0.0947\n",
            "Epoch 64/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.1250 - val_loss: 0.0938\n",
            "Epoch 65/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1230 - val_loss: 0.0927\n",
            "Epoch 66/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1144 - val_loss: 0.0914\n",
            "Epoch 67/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.1373 - val_loss: 0.0896\n",
            "Epoch 68/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.1397 - val_loss: 0.0870\n",
            "Epoch 69/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 0.1100 - val_loss: 0.0847\n",
            "Epoch 70/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 0.1077 - val_loss: 0.0826\n",
            "Epoch 71/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 0.1277 - val_loss: 0.0807\n",
            "Epoch 72/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 0.1104 - val_loss: 0.0794\n",
            "Epoch 73/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0901 - val_loss: 0.0792\n",
            "Epoch 74/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1172 - val_loss: 0.0798\n",
            "Epoch 75/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 0.1059 - val_loss: 0.0807\n",
            "Epoch 76/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.1087 - val_loss: 0.0818\n",
            "Epoch 77/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.1143 - val_loss: 0.0830\n",
            "Epoch 78/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.1060 - val_loss: 0.0850\n",
            "Epoch 79/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0818 - val_loss: 0.0873\n",
            "Epoch 80/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.1151 - val_loss: 0.0890\n",
            "Epoch 81/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 0.0974 - val_loss: 0.0907\n",
            "Epoch 82/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 0.1061 - val_loss: 0.0914\n",
            "Epoch 83/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.1017 - val_loss: 0.0911\n",
            "Epoch 84/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0859 - val_loss: 0.0907\n",
            "Epoch 85/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0892 - val_loss: 0.0884\n",
            "Epoch 86/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0912 - val_loss: 0.0859\n",
            "Epoch 87/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 0.0952 - val_loss: 0.0829\n",
            "Epoch 88/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 0.0847 - val_loss: 0.0801\n",
            "Epoch 89/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.0759 - val_loss: 0.0785\n",
            "Epoch 90/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 0.0852 - val_loss: 0.0777\n",
            "Epoch 91/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 0.0861 - val_loss: 0.0783\n",
            "Epoch 92/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 0.0970 - val_loss: 0.0786\n",
            "Epoch 93/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.0733 - val_loss: 0.0804\n",
            "Epoch 94/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0755 - val_loss: 0.0836\n",
            "Epoch 95/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 0.0955 - val_loss: 0.0852\n",
            "Epoch 96/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.0802 - val_loss: 0.0880\n",
            "Epoch 97/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 0.0709 - val_loss: 0.0907\n",
            "Epoch 98/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 0.0781 - val_loss: 0.0913\n",
            "Epoch 99/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0887 - val_loss: 0.0881\n",
            "Epoch 100/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.0727 - val_loss: 0.0816\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7a39c058b060> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 528ms/step"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-1' coro=<Server.serve() done, defined at /usr/local/lib/python3.11/dist-packages/uvicorn/server.py:68> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/main.py\", line 579, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 66, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 69, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 330, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n",
            "WARNING:tensorflow:6 out of the last 10 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7a39c058b060> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 919ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348ms/step\n",
            "INFO:     2409:40e3:2093:753a:a9c5:d501:107:3738:0 - \"POST /predict/ndvi HTTP/1.1\" 500 Internal Server Error\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/applications.py\", line 1054, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/applications.py\", line 112, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/errors.py\", line 187, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/errors.py\", line 165, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/cors.py\", line 93, in __call__\n",
            "    await self.simple_response(scope, receive, send, request_headers=headers)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/cors.py\", line 144, in simple_response\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 714, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 734, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 288, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 76, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 73, in app\n",
            "    response = await f(request)\n",
            "               ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/routing.py\", line 301, in app\n",
            "    raw_response = await run_endpoint_function(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/routing.py\", line 214, in run_endpoint_function\n",
            "    return await run_in_threadpool(dependant.call, **values)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/concurrency.py\", line 37, in run_in_threadpool\n",
            "    return await anyio.to_thread.run_sync(func)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/asyncio/futures.py\", line 287, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 349, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.11/asyncio/futures.py\", line 203, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-8-2c8045e262d0>\", line 11, in predict_ndvi\n",
            "    return JSONResponse(content={\"ndvi_values\": data})\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/responses.py\", line 182, in __init__\n",
            "    super().__init__(content, status_code, headers, media_type, background)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/responses.py\", line 45, in __init__\n",
            "    self.body = self.render(content)\n",
            "                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/responses.py\", line 185, in render\n",
            "    return json.dumps(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/json/__init__.py\", line 238, in dumps\n",
            "    **kw).encode(obj)\n",
            "          ^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/json/encoder.py\", line 200, in encode\n",
            "    chunks = self.iterencode(o, _one_shot=True)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/json/encoder.py\", line 258, in iterencode\n",
            "    return _iterencode(o, 0)\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/json/encoder.py\", line 180, in default\n",
            "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
            "TypeError: Object of type float32 is not JSON serializable\n"
          ]
        }
      ]
    }
  ]
}